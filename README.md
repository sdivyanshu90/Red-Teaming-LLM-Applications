# Red Teaming LLM Applications

This repository accompanies the **[Red Teaming LLM Applications](https://www.deeplearning.ai/short-courses/red-teaming-llm-applications/)**, created by **[DeepLearning.AI](https://www.deeplearning.ai/)** in collaboration with **[Giskard](https://www.giskard.ai/)**.

The course introduces practical methods for stress-testing large language model (LLM) applications, uncovering hidden vulnerabilities, and building safer, more trustworthy AI systems. **DeepLearning.AI** licenses the included notebook.

---

## Course Topics

### 1. Overview of LLM Vulnerabilities

LLMs can fail in subtle yet high-impact ways. This module highlights the most common risks, including:

* **Prompt injection** – adversarial prompts that override intended behavior.
* **Data leakage** – unintentional disclosure of sensitive or proprietary information.
* **Bias and toxicity** – perpetuation of harmful stereotypes or offensive content.
* **Hallucinations** – fabrication of facts or misleading outputs.

Understanding these vulnerabilities is the first step toward effective red teaming.

---

### 2. Red Teaming LLMs

Red teaming means actively challenging an AI system to expose weaknesses. In this section, you’ll learn how to:

* Adopt the mindset of an adversarial tester.
* Craft targeted prompts to reveal unsafe or unreliable outputs.
* Evaluate system behavior under edge cases and adversarial conditions.

You’ll gain a structured approach to probing LLM robustness beyond surface-level testing.

---

### 3. Red Teaming at Scale

Real-world applications require more than one-off tests. This module covers how to:

* Automate large volumes of adversarial tests.
* Apply systematic coverage strategies to explore model behavior broadly.
* Aggregate and interpret large-scale test results to uncover patterns in weaknesses.

Scaling red teaming ensures vulnerabilities don’t slip through unnoticed.

---

### 4. Red Teaming LLMs with LLMs

LLMs can also help red team other LLMs. This section demonstrates how to:

* Use LLMs to generate adversarial prompts automatically.
* Employ models as automated evaluators of output safety and quality.
* Integrate model-assisted red teaming into human-in-the-loop testing pipelines.

This creates a hybrid strategy that combines automation with expert judgment.

---

### 5. A Full Red Teaming Assessment

The final module synthesizes all concepts into a complete assessment workflow:

* Planning and executing an end-to-end red teaming exercise.
* Documenting vulnerabilities, risks, and proposed mitigations.
* Reporting findings to stakeholders in a clear, actionable way.
* Embedding continuous red teaming into the development lifecycle.

By the end, you’ll be equipped to conduct comprehensive evaluations of LLM applications.

---

## Acknowledgments

This course was developed by **[DeepLearning.AI](https://www.deeplearning.ai/)** in partnership with **[Giskard](https://www.giskard.ai/)**, combining expertise in AI education and AI safety.

Special thanks to the teams who contributed their knowledge and resources to make this course possible.
